# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-st7i0A4DPhd6bN6MHdxFOnm1-t77DGL
"""

# If you use kagglehub or already have the path, keep it; otherwise set 'path' to the dataset folder.
# Example in your screenshot: path = "/kaggle/input/imdb-movie-ratings-sentiment-analysis"
path = "/kaggle/input/imdb-movie-ratings-sentiment-analysis"

# --- Imports ---
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# --- Show files in dataset folder ---
print("Dataset folder:", path)
files = os.listdir(path)
for f in files:
    print(" -", f)

# --- Find a CSV file automatically (prefers 'movie.csv' if present) ---
csv_file = None
if "movie.csv" in files:
    csv_file = "movie.csv"
else:
    # pick first csv file
    for f in files:
        if f.lower().endswith(".csv"):
            csv_file = f
            break

if csv_file is None:
    raise FileNotFoundError(f"No CSV file found in {path}. Files: {files}")

full_csv_path = os.path.join(path, csv_file)
print("\nUsing CSV file:", full_csv_path)

# --- Try reading with a couple encodings if default fails ---
encodings_to_try = ["utf-8", "latin-1", "utf-16"]
df = None
for enc in encodings_to_try:
    try:
        df = pd.read_csv(full_csv_path, encoding=enc)
        print(f"Loaded CSV with encoding: {enc}")
        break
    except Exception as e:
        print(f"Failed with encoding {enc}: {e}")

if df is None:
    raise ValueError("Failed to read the CSV file with common encodings. Inspect file manually.")

# --- Quick peek ---
print("\nDataframe shape:", df.shape)
print(df.head())

# --- Identify likely columns ---
# Common Kaggle IMDB dataset columns: 'review' and 'sentiment'
possible_text_cols = [c for c in df.columns if c.lower() in ("review", "text", "comment", "content")]
possible_label_cols = [c for c in df.columns if c.lower() in ("sentiment", "label", "target")]

# If not identified automatically, fall back to first two columns
if not possible_text_cols:
    text_col = df.columns[0]
else:
    text_col = possible_text_cols[0]

if not possible_label_cols and len(df.columns) > 1:
    label_col = df.columns[1]
else:
    label_col = possible_label_cols[0]

print("\nUsing text column:", text_col)
print("Using label column:", label_col)

# --- Keep only text and label, drop NA ---
df = df[[text_col, label_col]].dropna()
df = df.reset_index(drop=True)

# --- Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(
    df[text_col], df[label_col], test_size=0.2, random_state=42, stratify=df[label_col] if len(df[label_col].unique())>1 else None
)

# --- Vectorize ---
vectorizer = CountVectorizer(stop_words="english", max_features=30000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# --- Model training ---
model = MultinomialNB()
model.fit(X_train_vec, y_train)

# --- Predict & evaluate ---
y_pred = model.predict(X_test_vec)
print("\nðŸŽ¯ Accuracy:", accuracy_score(y_test, y_pred))
print("\nðŸ“‰ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nðŸ“‹ Classification Report:\n", classification_report(y_test, y_pred))

# --- Custom samples ---
sample_reviews = [
    "This movie was fantastic! Loved the story and acting.",
    "Terrible film. Waste of time and money."
]
sample_vec = vectorizer.transform(sample_reviews)
print("\nCustom Predictions:")
for review, pred in zip(sample_reviews, model.predict(sample_vec)):
    print(f"{review} â†’ {pred}")